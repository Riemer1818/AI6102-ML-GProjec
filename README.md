# Evaluating Game-Playing Strength of MCTS Variants

This repository contains the code and documentation for our team project in AI6102: Machine Learning Methodologies & Applications, where we tackle the Kaggle competition [UM Game-Playing Strength of MCTS Variants](https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants). Our goal is to assess and enhance the game-playing strength of various Monte Carlo Tree Search (MCTS) algorithms, exploring the nuances and improvements within this AI approach for game strategies.

## Project Overview

The focus of this project is to:
- **Define** the problem of evaluating MCTS variants in competitive game settings.
- **Analyze** the unique challenges posed by different MCTS implementations.
- **Develop** and propose improvements to maximize the performance of MCTS-based agents.

## Motivation and Solution Approach

Our motivation for this competition stems from MCTS's critical role in modern game AI and its application in high-stakes environments like board games and strategy games. Through feature engineering and experimentation, we seek to optimize our selected MCTS variant by refining parameters and integrating advanced machine learning methods where appropriate. 

## Methodology and Experiments

Following a structured methodology, we:
1. Conducted feature engineering and hyperparameter tuning.
2. Tested different MCTS configurations to observe their effect on gameplay performance.
3. Evaluated our model's performance through cross-validation and competition leaderboard standings.

## Results and Conclusions

We document our findings, challenges, and improvements in this repository. The final report includes:
- **Cross-validation** and leaderboard results.
- **Key insights** gained from the experiment.
- Reflections on MCTS's limitations and potential future enhancements.

This repository serves as a comprehensive account of our process, code, and results, providing a resource for understanding and advancing MCTS-based AI strategies.
